##  IEEE-CIS თაღლითობის აღმოჩენის გამოწვევის მიმოხილვა

ეს პროექტი ეფუძნება Kaggle-ის IEEE-CIS Fraud Detection კონკურსს. მისი მიზანია ელექტრონული კომერციის სექტორში თაღლითობის პრევენციის სისტემების გაუმჯობესება — კონკრეტულად, თაღლითური ტრანზაქციების პროგნოზირება Vesta Corporation-ის მიერ მოწოდებულ რეალურ მონაცემებზე დაყრდნობით.

მონაცემთა ნაკრები მოიცავს დეტალურ ტრანზაქციებს მრავალფეროვანი მახასიათებლებით, რომლებიც იყოფა ორ ძირითად კატეგორიად:

- **ტრანზაქციის მახასიათებლები** – თითოეული გადახდის შესახებ ინფორმაცია  
- **იდენტობის მახასიათებლები** – დამატებითი ცნობები მომხმარებლისა და მოწყობილობის შესახებ

რეპოზიტორია შეიცავს რამდენიმე ნოუთბუქს, რომლებიც ასახავს მოდელირების პროცესის სხვადასხვა ასპექტს:

- **model_experiment_LogisticRegression.ipynb**: ლოგისტიკური რეგრესიის მოდელის იმპლემენტაცია და ოპტიმიზაცია
- **model_experiment_RandomForest.ipynb**: Random Forest მოდელის იმპლემენტაცია და ოპტიმიზაცია
- **model_experiment_XGBoost.ipynb**: XGBoost მოდელის იმპლემენტაცია და ოპტიმიზაცია
- **model_inference.ipynb**: საუკეთესო მოდელის ჩატვირთვა და სატესტო მონაცემებზე პროგნოზების გენერირება

## Feature Engineering

### კატეგორიული ცვლადების რიცხვითში გადაყვანა

ყველა კატეგორიული ცვლადი დაკოდირდა სხვადასხვა ტექნიკის გამოყენებით, მათი კარდინალობის მიხედვით:

- **Label Encoding**: დაბალი კარდინალობის მქონე კატეგორიული მახასიათებლებისთვის გამოყენებულ იქნა ძირითადი ლეიბელის
  კოდირება.
- **Frequency Encoding**: მაღალი კარდინალობის მახასიათებლებისთვის გამოყენებულ იქნა სიხშირული კოდირება მათი განაწილებების
  დასაფიქსირებლად.
- **სპეციალური დამუშავება**: ელფოსტის დომენებმა, მოწყობილობის ტიპებმა და სხვა სპეციალურმა კატეგორიულმა ცვლადებმა მიიღეს
  ინდივიდუალური კოდირება.

ეს პროცესი განხორციელდა სხვადასხვა ტრანსფორმერების გამოყენებით, როგორებიცაა `CategoryEncoder`, რომელიც სკიკიტ-ლერნისის
`BaseEstimator` და `TransformerMixin` კლასებზეა დაფუძნებული.

### Nan მნიშვნელობების დამუშავება

მონაცემთა ნაკრები შეიცავდა მნიშვნელოვანი რაოდენობის გამოტოვებულ მნიშვნელობებს. გამოყენებულ იქნა სხვადასხვა მიდგომა:

- **მაღალი გამოტოვების მქონე სვეტები**: მახასიათებლები, რომლებშიც გამოტოვებული იყო მონაცემების 80%-ზე მეტი, ამოღებულ
  იქნა.
- **რიცხვითი მახასიათებლები**: გამოტოვებული მნიშვნელობები ჩანაცვლდა -999-ით (როგორც სპეციალური ინდიკატორი).
- **კატეგორიული მახასიათებლები**: გამოტოვებული მნიშვნელობები განიხილებოდა როგორც ცალკე კატეგორია ("missing").

კლასი `MissingValueHandler` იქნა შექმნილი ამ პროცესის ავტომატიზაციისთვის, რომელიც ახდენს მაღალი გამოტოვების სვეტების
იდენტიფიცირებასა და შესაბამის დამუშავებას.

### Cleaning მიდგომები

გამოყენებულ იქნა მონაცემთა გაწმენდის რამდენიმე ტექნიკა:

- **Outlier დამუშავება**: ექსტრემალური მნიშვნელობები შეიზღუდა გარკვეულ პერცენტილებზე მათი გავლენის შესამცირებლად.
- **მუდმივი მახასიათებლები**: მახასიათებლები, რომლებსაც მხოლოდ ერთი უნიკალური მნიშვნელობა ჰქონდათ, ამოღებულ იქნა.
- **დუბლირებული მახასიათებლები**: იდენტიფიცირებულ და შემცირებულ იქნა მაღალი კორელაციის მქონე მახასიათებლები.

ამ მიდგომების განხორციელება მოხდა `OutlierHandler` და `ConstantFeatureFilter` კლასების საშუალებით.

## Feature Selection

გამოკვლეულ იქნა ყველაზე მნიშვნელოვანი მახასიათებლების შერჩევის რამდენიმე მიდგომა:

- **მახასიათებლების მნიშვნელოვნების ანალიზი**: მოდელზე დაფუძნებული მნიშვნელოვნების ქულების გამოყენება მთავარი
  მახასიათებლების იდენტიფიცირებისთვის.
- **კორელაციის ანალიზი**: მაღალი კორელაციის მქონე მახასიათებლების ამოღება რედუნდანტობის შესამცირებლად.
- **SelectKBest**: სტატისტიკური ტესტები ყველაზე მნიშვნელოვანი K მახასიათებლის შესარჩევად, მათი სამიზნე ცვლადთან
  ურთიერთკავშირის საფუძველზე.

ახალი მახასიათებლები შეიქმნა ტრანზაქციის დროის ინფორმაციიდან, როგორიცაა:

- კვირის დღე
- დღის საათი
- შაბათ-კვირის ინდიკატორი
- სხვადასხვა მახასიათებლებს შორის ინტერაქციები

მახასიათებლების შერჩევა მოხდა `FeatureSelector` კლასის გამოყენებით, რომელიც 100 საუკეთესო მახასიათებლის შერჩევას ახდენდა
სხვადასხვა სტატისტიკური ტესტების საფუძველზე.

## Training

### ტესტირებული მოდელები

საფუძვლიანად გამოიცადა სამი ძირითადი მოდელი:

1. **ლოგისტიკური რეგრესია**:

- საბაზისო მოდელი L1/L2 რეგულარიზაციით
- სწრაფი ტრენინგი და ინტერპრეტირებადი
- ROC-AUC: 0.8232, PR-AUC: 0.3216

2. **Random Forest**:

- ხეზე დაფუძნებული ანსამბლური მოდელი
- კარგად უმკლავდება არაწრფივ ურთიერთკავშირებს
- ROC-AUC: 0.8827, PR-AUC: 0.5213

3. **XGBoost**:

- გრადიენტის გაძლიერების მოდელი
- საუკეთესო ეფექტურობა
- ROC-AUC: 0.9444, PR-AUC: 0.6786

### Hyperparameter ოპტიმიზაციის მიდგომა

თითოეული მოდელისთვის განხორციელდა ჰიპერპარამეტრების მოწყობის სტრუქტურირებული პროცესი:

- **ლოგისტიკური რეგრესია**: რეგულარიზაციის სიძლიერე (C), სასჯელის ტიპი (L1, L2, elasticnet)
- **Random Forest**: ესტიმატორების რაოდენობა, მაქსიმალური სიღრმე
- **XGBoost**: სწავლის სიჩქარე, ესტიმატორების რაოდენობა, მაქსიმალური სიღრმე, subsample კოეფიციენტი

მოდელების შეფასება მოხდა როგორც ROC-AUC მეტრიკით, ასევე PR-AUC მეტრიკით, რომელიც უფრო შესაფერისია არაბალანსირებული
დატასეტების შემთხვევაში.

### საბოლოო მოდელის შერჩევის დასაბუთება

XGBoost მოდელი შეირჩა საბოლოო მოდელად შემდეგი მიზეზების გამო:

1. **უკეთესი ეფექტურობა**: უმაღლესი ROC-AUC (0.9444) და PR-AUC (0.6786) ქულები.
2. **არაბალანსირებული მონაცემების კარგი დამუშავება**: უკეთესი ეფექტურობა არაბალანსირებულ თაღლითობის მონაცემთა ნაკრებთან.
3. **მახასიათებლების მნიშვნელოვნება**: მკაფიო მახასიათებლების მნიშვნელოვნების ქულები.
4. **სიმყარე**: ნაკლები overfitting Random Forest-თან შედარებით და უკეთესი განზოგადება ვიდრე ლოგისტიკურ რეგრესიას.

## MLflow Tracking

### MLflow ექსპერიმენტების ბმული

ყველა ექსპერიმენტი თვალყურისდევნებული იყო MLflow-ით და ხელმისაწვდომია:
https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow

### ჩაწერილი მეტრიკები

თითოეული მოდელისთვის თვალყური ედევნებოდა რამდენიმე მეტრიკას:

- **ROC-AUC**: ROC მრუდის ქვეშ არეა, რომელიც ზომავს რანჟირების საერთო უნარს
- **PR-AUC**: პრეციზიულობა-გამოძავების მრუდის ქვეშ არეა
- **მოდელის პარამეტრები**: ყველა ჰიპერპარამეტრი, რომელიც გამოყენებულია თითოეულ ექსპერიმენტში
- **მახასიათებლების მნიშვნელოვნება**: ყველაზე მნიშვნელოვანი მახასიათებლები ხეზე დაფუძნებული მოდელებისთვის

### საუკეთესო მოდელის შედეგები

საუკეთესო მოდელმა (XGBoost) მიაღწია:

- ROC-AUC: 0.9444
- PR-AUC: 0.6786
- პროგნოზების განაწილება სატესტო მონაცემებზე:
- საშუალო პროგნოზი: 0.020070
- დიაპაზონი: [0.000009, 0.991089]
- > 0.5 დროშების რაოდენობა: 3444 (0.68%)

## ძირითადი მიგნებები და ანალიზი

### მოდელის ეფექტურობის ანალიზი

- **ლოგისტიკური რეგრესია**: underfitting მონაცემებზე, რაც იმაზე მიუთითებს, რომ თაღლითობის მოდელები არაწრფივი და რთულია.
  მიუხედავად იმისა, რომ ეს მოდელი ყველაზე სწრაფად ტრენირდება, მისი ეფექტურობა მნიშვნელოვნად ჩამოუვარდება სხვა მოდელებს,
  განსაკუთრებით PR-AUC მეტრიკის მიხედვით.

- **Random Forest**: კარგი ეფექტურობა, მაგრამ ღრმა ხეებთან overfitting-ის ნიშნები გამოავლინა. როდესაც ხეების სიღრმე
  გაიზარდა 10-დან 20-მდე, მოდელმა დაიწყო overfitting, რაც გამოვლინდა ვალიდაციის ნაკრებზე ეფექტურობის გაუარესებით.
  საუკეთესო ბალანსი მიღწეულ იქნა 15 სიღრმის და 200 ესტიმატორის გამოყენებით.

- **XGBoost**: საუკეთესო ბალანსი ტრენინგის მონაცემების მორგებასა და დაუნახავ მონაცემებზე განზოგადებას შორის. XGBoost-მა
  აჩვენა უკეთესი უნარი დისკრიმინაციისთვის დაბალი სიხშირის, მაგრამ მაღალი რისკის თაღლითობის შემთხვევებს შორის, რაც აისახა
  PR-AUC-ის უფრო მაღალ მაჩვენებელში.

### მახასიათებლების მნიშვნელოვნების ანალიზი

ყველაზე მნიშვნელოვანი მახასიათებლები მოდელებში იყო:

1. ტრანზაქციის თანხა და მისი ლოგარითმული ტრანსფორმაცია
2. ბარათთან დაკავშირებული მახასიათებლები (card1, card2, card3)
3. ელფოსტის დომენების თანხვედრა მყიდველსა და მიმღებს შორის
4. მოწყობილობის ტიპის ინფორმაცია
5. დროზე დაფუძნებული მახასიათებლები (კვირის დღე, დღის საათი)

საინტერესოა, რომ ყველა მოდელისთვის მნიშვნელოვანი იყო თაღლითობის ამოსაცნობად როგორც ფინანსური ინფორმაცია (ტრანზაქციის
თანხა), ასევე ქცევითი ფაქტორები (დროის მახასიათებლები, მოწყობილობის ინფორმაცია).

### კლასების დისბალანსის დამუშავება

გამოყენებულ იქნა რამდენიმე ტექნიკა მწვავე კლასთა დისბალანსის გადასაჭრელად (3.5% თაღლითობა):

- XGBoost-ში scale_pos_weight პარამეტრის გამოყენება უმცირესობის კლასის მნიშვნელობის გასაზრდელად
- PR-AUC-ით შეფასება (უფრო შესაფერისია არაბალანსირებული კლასიფიკაციისთვის)
- ცრუ უარყოფითი შედეგების შემცირებაზე ფოკუსირება თაღლითობის შემთხვევებისთვის


