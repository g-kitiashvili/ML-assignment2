{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:14.792371Z",
     "start_time": "2025-04-22T15:56:12.854499Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -q dagshub mlflow",
   "id": "29e14893778177b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import/setup",
   "id": "b4b035e328bec4bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'g-kitiashvili'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '1c2227158cc19daf66bb3b241116a8e8c5f1cd20' \n",
    "\n",
    "model_name = \"LogisticRegression\" \n",
    "mlflow.set_experiment(f\"{model_name}_Training\")\n"
   ],
   "id": "f231bc57be8e2792"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Custom Transformers for Pipeline"
   ],
   "id": "2e90c041c2c354c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:34:54.341746Z",
     "start_time": "2025-04-23T11:34:54.304851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, k=100):\n",
    "        self.k = k\n",
    "        self.selector = None\n",
    "        self.selected_feature_indices = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.selector = SelectKBest(f_classif, k=min(self.k, X.shape[1]))\n",
    "        self.selector.fit(X, y)\n",
    "        self.selected_feature_indices = self.selector.get_support()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, threshold=50):\n",
    "        self.threshold = threshold\n",
    "        self.high_missing_cols = None\n",
    "        self.constant_features = None\n",
    "        self.medians = {}\n",
    "        self.modes = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Identify columns with too many missing values\n",
    "        missing_percent = (X.isnull().mean() * 100)\n",
    "        self.high_missing_cols = missing_percent[missing_percent > self.threshold].index.tolist()\n",
    "        \n",
    "        # Identify constant features\n",
    "        self.constant_features = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "        \n",
    "        # Calculate median for numerical features\n",
    "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            if col not in self.high_missing_cols and col not in self.constant_features:\n",
    "                self.medians[col] = X[col].median()\n",
    "        \n",
    "        # Calculate mode for categorical features\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if col not in self.high_missing_cols and col not in self.constant_features:\n",
    "                self.modes[col] = X[col].mode()[0] if not X[col].mode().empty else 'missing'\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Remove high missing columns\n",
    "        X_new = X_new.drop(columns=self.high_missing_cols, errors='ignore')\n",
    "        \n",
    "        # Remove constant features\n",
    "        X_new = X_new.drop(columns=self.constant_features, errors='ignore')\n",
    "        \n",
    "        # Fill missing values in numerical features\n",
    "        for col, median in self.medians.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[col] = X_new[col].fillna(median)\n",
    "        \n",
    "        # Fill missing values in categorical features\n",
    "        for col, mode in self.modes.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[col] = X_new[col].fillna(mode)\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, q_low=0.01, q_high=0.99):\n",
    "        self.q_low = q_low\n",
    "        self.q_high = q_high\n",
    "        self.bounds = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            if 'TransactionDT' not in col and col in X.columns:\n",
    "                q1 = X[col].quantile(self.q_low)\n",
    "                q3 = X[col].quantile(self.q_high)\n",
    "                self.bounds[col] = (q1, q3)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        for col, (q1, q3) in self.bounds.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[col] = np.clip(X_new[col], q1, q3)\n",
    "        return X_new\n",
    "\n",
    "class DatetimeFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'TransactionDT' in X_new.columns:\n",
    "            # Convert to days\n",
    "            X_new['TransactionDay'] = X_new['TransactionDT'] / (24 * 60 * 60)\n",
    "            \n",
    "            # Create cyclical features for day of week\n",
    "            X_new['DayOfWeek_sin'] = np.sin(2 * np.pi * (X_new['TransactionDay'] % 7) / 7)\n",
    "            X_new['DayOfWeek_cos'] = np.cos(2 * np.pi * (X_new['TransactionDay'] % 7) / 7)\n",
    "            \n",
    "            # Create hour of day\n",
    "            X_new['Hour'] = (X_new['TransactionDT'] % (24 * 60 * 60)) / 3600\n",
    "            \n",
    "            # Cyclical features for hour\n",
    "            X_new['Hour_sin'] = np.sin(2 * np.pi * X_new['Hour'] / 24)\n",
    "            X_new['Hour_cos'] = np.cos(2 * np.pi * X_new['Hour'] / 24)\n",
    "            \n",
    "            # Drop original\n",
    "            X_new = X_new.drop(['TransactionDT', 'Hour'], axis=1)\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'TransactionAmt' in X_new.columns:\n",
    "            X_new['TransactionAmt_Log'] = np.log1p(X_new['TransactionAmt'])\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class CategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, max_categories=20):\n",
    "        self.max_categories = max_categories\n",
    "        self.encodings = {}\n",
    "        self.frequency_maps = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if X[col].nunique() < self.max_categories:\n",
    "                # Get dummies for low cardinality\n",
    "                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n",
    "                self.encodings[col] = dummies.columns.tolist()\n",
    "            else:\n",
    "                # Frequency encoding for high cardinality\n",
    "                self.frequency_maps[col] = X[col].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Apply encodings\n",
    "        for col, dummy_cols in self.encodings.items():\n",
    "            if col in X_new.columns:\n",
    "                # Create one-hot encoding\n",
    "                dummies = pd.get_dummies(X_new[col], prefix=col, drop_first=True)\n",
    "                \n",
    "                # Ensure all expected columns exist\n",
    "                for dummy_col in dummy_cols:\n",
    "                    if dummy_col not in dummies.columns:\n",
    "                        dummies[dummy_col] = 0\n",
    "                \n",
    "                # Only keep columns from training\n",
    "                dummies = dummies[dummy_cols]\n",
    "                \n",
    "                # Add to dataframe\n",
    "                X_new = pd.concat([X_new, dummies], axis=1)\n",
    "                \n",
    "                # Drop original\n",
    "                X_new = X_new.drop(columns=[col])\n",
    "        \n",
    "        # Apply frequency encoding\n",
    "        for col, freq_map in self.frequency_maps.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[f'{col}_freq'] = X_new[col].map(freq_map).fillna(0)\n",
    "                X_new = X_new.drop(columns=[col])\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class InteractionFeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.interaction_features = []\n",
    "        self.card_cols = []\n",
    "        self.addr_cols = []\n",
    "        self.email_cols = []\n",
    "        \n",
    "    def _clean_feature_name(self, name):\n",
    "        if isinstance(name, str):\n",
    "            # Replace spaces, special chars with underscores\n",
    "            return name.replace(' ', '_').replace('-', '_').replace('/', '_')\n",
    "        return str(name)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Store the original column lists for later use\n",
    "        self.card_cols = [col for col in X.columns if 'card' in col.lower() \n",
    "                     and X[col].dtype != 'object' and not pd.api.types.is_bool_dtype(X[col])]\n",
    "        \n",
    "        self.addr_cols = [col for col in X.columns if 'addr' in col.lower() \n",
    "                     and X[col].dtype != 'object' and not pd.api.types.is_bool_dtype(X[col])]\n",
    "        \n",
    "        self.email_cols = [col for col in X.columns if 'email' in col.lower() \n",
    "                      and X[col].dtype != 'object' and not pd.api.types.is_bool_dtype(X[col])]\n",
    "        \n",
    "        # Pre-compute which interaction features will be created\n",
    "        self.interaction_features = []\n",
    "        \n",
    "        # Card interactions\n",
    "        if len(self.card_cols) >= 2:\n",
    "            for i in range(min(3, len(self.card_cols)-1)):\n",
    "                for j in range(i+1, min(i+3, len(self.card_cols))):\n",
    "                    col_name = f\"{self.card_cols[i]}_x_{self.card_cols[j]}\"\n",
    "                    self.interaction_features.append(col_name)\n",
    "        \n",
    "        # Address interactions\n",
    "        if len(self.addr_cols) >= 2:\n",
    "            for i in range(min(3, len(self.addr_cols)-1)):\n",
    "                for j in range(i+1, min(i+3, len(self.addr_cols))):\n",
    "                    col_name = f\"{self.addr_cols[i]}_x_{self.addr_cols[j]}\"\n",
    "                    self.interaction_features.append(col_name)\n",
    "        \n",
    "        # Email interactions\n",
    "        if len(self.email_cols) >= 2:\n",
    "            for i in range(len(self.email_cols)-1):\n",
    "                for j in range(i+1, len(self.email_cols)):\n",
    "                    col_name = f\"{self.email_cols[i]}_x_{self.email_cols[j]}\"\n",
    "                    self.interaction_features.append(col_name)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Create card interactions\n",
    "        if len(self.card_cols) >= 2:\n",
    "            for i in range(min(3, len(self.card_cols)-1)):\n",
    "                for j in range(i+1, min(i+3, len(self.card_cols))):\n",
    "                    # Only create features that were in the training set\n",
    "                    col_name = f\"{self.card_cols[i]}_x_{self.card_cols[j]}\"\n",
    "                    if col_name in self.interaction_features:\n",
    "                        # Ensure both columns exist in X\n",
    "                        if self.card_cols[i] in X_new.columns and self.card_cols[j] in X_new.columns:\n",
    "                            X_new[col_name] = X_new[self.card_cols[i]] * X_new[self.card_cols[j]]\n",
    "        \n",
    "        # Create addr interactions\n",
    "        if len(self.addr_cols) >= 2:\n",
    "            for i in range(min(3, len(self.addr_cols)-1)):\n",
    "                for j in range(i+1, min(i+3, len(self.addr_cols))):\n",
    "                    col_name = f\"{self.addr_cols[i]}_x_{self.addr_cols[j]}\"\n",
    "                    if col_name in self.interaction_features:\n",
    "                        if self.addr_cols[i] in X_new.columns and self.addr_cols[j] in X_new.columns:\n",
    "                            X_new[col_name] = X_new[self.addr_cols[i]] * X_new[self.addr_cols[j]]\n",
    "        \n",
    "        # Create email domain interactions\n",
    "        if len(self.email_cols) >= 2:\n",
    "            for i in range(len(self.email_cols)-1):\n",
    "                for j in range(i+1, len(self.email_cols)):\n",
    "                    col_name = f\"{self.email_cols[i]}_x_{self.email_cols[j]}\"\n",
    "                    if col_name in self.interaction_features:\n",
    "                        if self.email_cols[i] in X_new.columns and self.email_cols[j] in X_new.columns:\n",
    "                            X_new[col_name] = X_new[self.email_cols[i]] * X_new[self.email_cols[j]]\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, threshold=0.9):\n",
    "        self.threshold = threshold\n",
    "        self.drop_cols = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = X.corr().abs()\n",
    "        \n",
    "        # Get upper triangle\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features with correlation greater than threshold\n",
    "        self.drop_cols = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.drop_cols, errors='ignore')\n"
   ],
   "id": "8072e65d8ce01371",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Data Loading and Preparation\n",
   "id": "e248766a79565025"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:35:26.924047Z",
     "start_time": "2025-04-23T11:34:55.899663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "train_transaction = pd.read_csv('./data/train_transaction.csv')\n",
    "test_transaction = pd.read_csv('./data/test_transaction.csv')\n",
    "\n",
    "train_identity = pd.read_csv('./data/train_identity.csv')\n",
    "test_identity = pd.read_csv('./data/test_identity.csv')\n",
    "\n",
    "print(f\"Train transaction shape: {train_transaction.shape}\")\n",
    "print(f\"Test transaction shape: {test_transaction.shape}\")\n",
    "print(f\"Train identity shape: {train_identity.shape}\")\n",
    "print(f\"Test identity shape: {test_identity.shape}\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Initial_Preparation\") as run:\n",
    "    print(\"Merging data...\")\n",
    "    \n",
    "    train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "    test = test_transaction.merge(test_identity, on='TransactionID', how='left')\n",
    "    \n",
    "    mlflow.log_param(\"train_original_shape\", train.shape)\n",
    "    mlflow.log_param(\"test_original_shape\", test.shape)\n",
    "    \n",
    "    del train_transaction, train_identity\n",
    "    gc.collect()\n",
    "    \n",
    "    target = 'isFraud'\n",
    "    y_train = train[target].copy()\n",
    "    train_transaction_id = train['TransactionID'].copy()\n",
    "    test_transaction_id = test['TransactionID'].copy()\n",
    "    \n",
    "    fraud_ratio = y_train.mean()\n",
    "    mlflow.log_param(\"fraud_ratio\", fraud_ratio)\n",
    "    print(f\"Fraud ratio: {fraud_ratio:.4f}\")\n",
    "    \n",
    "    X_train = train.drop(['isFraud'], axis=1)\n",
    "    \n",
    "    del train\n",
    "    gc.collect()"
   ],
   "id": "cea77ac29ef80951",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train transaction shape: (590540, 394)\n",
      "Test transaction shape: (506691, 393)\n",
      "Train identity shape: (144233, 41)\n",
      "Test identity shape: (141907, 41)\n",
      "Merging data...\n",
      "Fraud ratio: 0.0350\n",
      "🏃 View run LogisticRegression_Initial_Preparation at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/722b5878acff4c03ab3c6ce62a9a1a7f\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " \n",
    " # Data Exploration for Pipeline Development\n"
   ],
   "id": "688a1dd4b01aea82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:35:53.459621Z",
     "start_time": "2025-04-23T11:35:31.626407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Exploration\") as run:\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    numerical_features.remove('TransactionID')  # Remove ID column\n",
    "    \n",
    "    mlflow.log_param(\"categorical_features_count\", len(categorical_features))\n",
    "    mlflow.log_param(\"numerical_features_count\", len(numerical_features))\n",
    "    \n",
    "    missing_values = X_train.isnull().mean() * 100\n",
    "    high_missing_cols = missing_values[missing_values > 50].index.tolist()\n",
    "    \n",
    "    mlflow.log_param(\"high_missing_cols_count\", len(high_missing_cols))\n",
    "    \n",
    "    mlflow.log_param(\"transaction_amount_mean\", X_train['TransactionAmt'].mean())\n",
    "    mlflow.log_param(\"transaction_amount_std\", X_train['TransactionAmt'].std())\n",
    "    \n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    print(f\"High missing columns: {len(high_missing_cols)}\")"
   ],
   "id": "d866c8b200fc4372",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: 31\n",
      "Numerical features: 401\n",
      "High missing columns: 214\n",
      "🏃 View run LogisticRegression_Exploration at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/5f8d83e032df43758700ce6e977f7f1a\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6a2b0b513ec56900"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating pipeline",
   "id": "654e852c8242440"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:36:15.541984Z",
     "start_time": "2025-04-23T11:36:00.288832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Pipeline_Testing\") as run:\n",
    "    \n",
    "    print(\"Testing MissingValueHandler...\")\n",
    "    missing_handler = MissingValueHandler(threshold=50)\n",
    "    missing_handler.fit(X_train)\n",
    "    X_sample = missing_handler.transform(X_train.iloc[:1000])\n",
    "    mlflow.log_param(\"missing_handler_removed_cols\", len(missing_handler.high_missing_cols) + len(missing_handler.constant_features))\n",
    "    \n",
    "    print(\"Testing DatetimeFeatureTransformer...\")\n",
    "    dt_transformer = DatetimeFeatureTransformer()\n",
    "    X_sample = dt_transformer.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing LogTransformer...\")\n",
    "    log_transformer = LogTransformer()\n",
    "    X_sample = log_transformer.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing OutlierHandler...\")\n",
    "    outlier_handler = OutlierHandler()\n",
    "    outlier_handler.fit(X_train)\n",
    "    X_sample = outlier_handler.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing CategoryEncoder...\")\n",
    "    cat_encoder = CategoryEncoder()\n",
    "    cat_encoder.fit(X_train)\n",
    "    X_sample = cat_encoder.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing InteractionFeatureCreator...\")\n",
    "    interaction_creator = InteractionFeatureCreator()\n",
    "    X_sample = interaction_creator.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing CorrelationFilter...\")\n",
    "    corr_filter = CorrelationFilter()\n",
    "    try:\n",
    "        corr_filter.fit(X_sample)\n",
    "        X_sample = corr_filter.transform(X_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"CorrelationFilter test error: {e}\")\n",
    "    \n",
    "    print(\"Testing FeatureSelector...\")\n",
    "    feature_selector = FeatureSelector(k=100)\n",
    "    try:\n",
    "        y_sample = y_train.iloc[:1000]\n",
    "        feature_selector.fit(X_sample, y_sample)\n",
    "        X_sample = feature_selector.transform(X_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"FeatureSelector test error: {e}\")\n",
    "    \n",
    "    mlflow.log_param(\"pipeline_testing_successful\", True)\n",
    "    mlflow.log_param(\"sample_features_after_transforms\", X_sample.shape[1])\n",
    "\n",
    "    \n",
    "def create_logistic_regression_pipeline(C=1.0, penalty='l2', class_weight='balanced', \n",
    "                                      solver='saga', l1_ratio=0.5, max_iter=1000):\n",
    "    preprocessor = Pipeline([\n",
    "        ('missing_handler', MissingValueHandler(threshold=50)),\n",
    "        ('datetime_transformer', DatetimeFeatureTransformer()),\n",
    "        ('log_transformer', LogTransformer()),\n",
    "        ('outlier_handler', OutlierHandler(q_low=0.01, q_high=0.99)),\n",
    "        ('category_encoder', CategoryEncoder(max_categories=20)),\n",
    "        ('interaction_creator', InteractionFeatureCreator()),\n",
    "        ('corr_filter', CorrelationFilter(threshold=0.9)),\n",
    "    ])\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        class_weight=class_weight,\n",
    "        solver=solver,\n",
    "        l1_ratio=l1_ratio if penalty == 'elasticnet' else None,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selector', FeatureSelector(k=100)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    return full_pipeline"
   ],
   "id": "7261ddebd42af02f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MissingValueHandler...\n",
      "Testing DatetimeFeatureTransformer...\n",
      "Testing LogTransformer...\n",
      "Testing OutlierHandler...\n",
      "Testing CategoryEncoder...\n",
      "Testing InteractionFeatureCreator...\n",
      "Testing CorrelationFilter...\n",
      "Testing FeatureSelector...\n",
      "🏃 View run LogisticRegression_Pipeline_Testing at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/605b01dd54b3408c805346bcca98d49b\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Data Preprocessing and training"
   ],
   "id": "2766dea9fe607999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:36:59.024307Z",
     "start_time": "2025-04-23T11:36:59.010904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(X_train, y_train, X_val=None, y_val=None, C=1.0, penalty='l2', max_iter=1000):\n",
    "    \n",
    "    preprocessor = Pipeline([\n",
    "        ('missing_handler', MissingValueHandler(threshold=50)),\n",
    "        ('datetime_transformer', DatetimeFeatureTransformer()),\n",
    "        ('log_transformer', LogTransformer()),\n",
    "        ('outlier_handler', OutlierHandler(q_low=0.01, q_high=0.99)),\n",
    "        ('category_encoder', CategoryEncoder(max_categories=20)),\n",
    "        ('interaction_creator', InteractionFeatureCreator()),\n",
    "        ('corr_filter', CorrelationFilter(threshold=0.9)),\n",
    "    ])\n",
    "    \n",
    "    print(\"Preprocessing data with sklearn pipeline...\")\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_val_preprocessed = preprocessor.transform(X_val) if X_val is not None else None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_preprocessed)\n",
    "    X_val_scaled = scaler.transform(X_val_preprocessed) if X_val_preprocessed is not None else None\n",
    "    \n",
    "    selector = FeatureSelector(k=100)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_val_selected = selector.transform(X_val_scaled) if X_val_scaled is not None else None\n",
    "    \n",
    "\n",
    "        \n",
    "    model = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty if penalty != 'elasticnet' else 'l1',  # elasticnet not supported without saga\n",
    "            solver='saga' if penalty == 'elasticnet' else 'liblinear',\n",
    "            l1_ratio=0.5 if penalty == 'elasticnet' else None,\n",
    "            max_iter=max_iter,\n",
    "            random_state=42\n",
    "        )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "        \n",
    "    if X_val_selected is not None:\n",
    "        y_val_pred = model.predict_proba(X_val_selected)[:, 1]\n",
    "    \n",
    "    if X_val_selected is not None:\n",
    "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_val_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        print(f\"Validation PR-AUC: {pr_auc:.4f}\")\n",
    "        \n",
    "        return model, preprocessor, scaler, selector, val_auc, pr_auc\n",
    "    \n",
    "    return model, preprocessor, scaler, selector, None, None"
   ],
   "id": "cc77afe6320f99fe",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CrossValidation and tuning",
   "id": "29788617d4e41135"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:44:42.940233Z",
     "start_time": "2025-04-23T11:37:01.417475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Cross_Validation\") as run:\n",
    "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    mlflow.log_param(\"validation_split\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"train_size\", X_train_cv.shape[0])\n",
    "    mlflow.log_param(\"validation_size\", X_val.shape[0])\n",
    "    \n",
    "    model, preprocessor, scaler, selector, val_auc, pr_auc = train(\n",
    "        X_train_cv, y_train_cv, X_val, y_val, C=1.0, penalty='l2', max_iter=1000\n",
    "    )\n",
    "    \n",
    "    mlflow.log_metric(\"validation_auc\", val_auc)\n",
    "    mlflow.log_metric(\"validation_pr_auc\", pr_auc)\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"cuML_LogisticRegression\")\n",
    "    mlflow.log_param(\"C\", 1.0)\n",
    "    mlflow.log_param(\"penalty\", \"l2\")\n",
    "    mlflow.log_param(\"max_iter\", 1000)\n",
    "    \n",
    "    print(f\"GPU-accelerated training complete.\")\n",
    "    print(f\"AUC: {val_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Hyperparameter_Tuning\") as run:\n",
    "    C_values = [0.01, 0.1, 1.0, 10.0]\n",
    "    \n",
    "    mlflow.log_param(\"tuning_C_values\", C_values)\n",
    "    \n",
    "    X_tune, X_val_tune, y_tune, y_val_tune = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_C = None\n",
    "    best_components = None\n",
    "    \n",
    "    for C in C_values:\n",
    "        print(f\"Testing C={C}...\")\n",
    "        \n",
    "        model, preprocessor, scaler, selector, val_auc, pr_auc = train(\n",
    "            X_tune, y_tune, X_val_tune, y_val_tune, C=C, penalty='l2', max_iter=1000\n",
    "        )\n",
    "        \n",
    "        mlflow.log_metric(f\"C_{C}_auc\", val_auc)\n",
    "        mlflow.log_metric(f\"C_{C}_pr_auc\", pr_auc)\n",
    "        \n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_C = C\n",
    "            best_components = (model, preprocessor, scaler, selector)\n",
    "    \n",
    "    mlflow.log_param(\"best_C\", best_C)\n",
    "    mlflow.log_metric(\"best_cv_auc\", best_auc)\n",
    "    best_params = {\n",
    "    'classifier__C': best_C,\n",
    "    'classifier__penalty': 'l2', \n",
    "    'classifier__class_weight': 'balanced'  \n",
    "    \n",
    "}\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best C value: {best_C}\")\n",
    "    print(f\"Best CV AUC: {best_auc:.4f}\")\n"
   ],
   "id": "83f82ba2f2a02377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data with sklearn pipeline...\n",
      "Validation AUC: 0.8232\n",
      "Validation PR-AUC: 0.3216\n",
      "GPU-accelerated training complete.\n",
      "AUC: 0.8232\n",
      "PR AUC: 0.3216\n",
      "🏃 View run LogisticRegression_Cross_Validation at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/b5029fd46f074b4d811eea076579a08d\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n",
      "Testing C=0.01...\n",
      "Preprocessing data with sklearn pipeline...\n",
      "Validation AUC: 0.8232\n",
      "Validation PR-AUC: 0.3214\n",
      "Testing C=0.1...\n",
      "Preprocessing data with sklearn pipeline...\n",
      "Validation AUC: 0.8232\n",
      "Validation PR-AUC: 0.3215\n",
      "Testing C=1.0...\n",
      "Preprocessing data with sklearn pipeline...\n",
      "Validation AUC: 0.8232\n",
      "Validation PR-AUC: 0.3216\n",
      "Testing C=10.0...\n",
      "Preprocessing data with sklearn pipeline...\n",
      "Validation AUC: 0.8232\n",
      "Validation PR-AUC: 0.3216\n",
      "Best parameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__class_weight': 'balanced'}\n",
      "Best C value: 1.0\n",
      "Best CV AUC: 0.8232\n",
      "🏃 View run LogisticRegression_Hyperparameter_Tuning at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/6eec609b2af5408d8eafe20bae48a5c7\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a8ffb46fb02bb726"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final Training with Full Dataset\n",
   "id": "dc5cfc7bab0ffdf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:47:44.669050Z",
     "start_time": "2025-04-23T11:44:50.604647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Final_Training\") as run:\n",
    "    try:\n",
    "        final_params = {param.replace('classifier__', ''): value \n",
    "                       for param, value in best_params.items()}\n",
    "    except NameError:\n",
    "        print(\"best_params not found. Using default parameters.\")\n",
    "        final_params = {\n",
    "            'C': 1.0,\n",
    "            'penalty': 'l2',\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "    \n",
    "    mlflow.log_params(final_params)\n",
    "    \n",
    "  \n",
    "    final_pipeline = create_logistic_regression_pipeline(\n",
    "        C=final_params.get('C', 1.0),\n",
    "        penalty=final_params.get('penalty', 'l2'),\n",
    "        class_weight=final_params.get('class_weight', 'balanced'),\n",
    "        solver='saga',\n",
    "        l1_ratio=0.5 if final_params.get('penalty') == 'elasticnet' else None,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    print(\"Training final model on full dataset...\")\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    try:\n",
    "        print(\"Testing pipeline on test data...\")\n",
    "        \n",
    "        original_test_columns = test.columns.tolist()\n",
    "        \n",
    "        columns_to_keep = [col for col in original_test_columns if col in X_train.columns]\n",
    "        test_matched = test[columns_to_keep]\n",
    "        \n",
    "        for col in X_train.columns:\n",
    "            if col not in test_matched.columns:\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    test_matched[col] = X_train[col].median()\n",
    "                else:\n",
    "                    test_matched[col] = X_train[col].mode()[0]\n",
    "        \n",
    "        test_probs = final_pipeline.predict_proba(test_matched)[:, 1]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        print(\"Falling back to basic prediction approach\")\n",
    "        simple_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(C=1.0, max_iter=1000))\n",
    "        ])\n",
    "        \n",
    "        numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "        X_train_simple = X_train[numeric_cols].copy()\n",
    "        test_simple = test[numeric_cols.intersection(test.columns)].copy()\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col not in test_simple.columns:\n",
    "                test_simple[col] = 0\n",
    "        \n",
    "        test_simple = test_simple[X_train_simple.columns]\n",
    "        \n",
    "        X_train_simple.fillna(0, inplace=True)\n",
    "        test_simple.fillna(0, inplace=True)\n",
    "        \n",
    "        simple_pipeline.fit(X_train_simple, y_train)\n",
    "        test_probs = simple_pipeline.predict_proba(test_simple)[:, 1]\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'TransactionID': test_transaction_id,\n",
    "        'isFraud': test_probs\n",
    "    })\n",
    "    \n",
    "    submission_file = f\"logistic_regression_submission_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    \n",
    "    mlflow.log_artifact(submission_file)\n",
    "    \n",
    "    mlflow.sklearn.log_model(final_pipeline, \"final_pipeline\")\n",
    "    \n",
    "    model_registry_name = f\"{model_name}_Pipeline\"\n",
    "    model_description = f\"Full {model_name} pipeline including all preprocessing steps\"\n",
    "    \n",
    "    try:\n",
    "        mlflow.register_model(\n",
    "            f\"runs:/{run.info.run_id}/final_pipeline\",\n",
    "            model_registry_name,\n",
    "            tags={\"description\": model_description}\n",
    "        )\n",
    "        print(f\"Final model registered as: {model_registry_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error registering model: {e}\")\n",
    "    \n",
    "    print(f\"Submission file saved as: {submission_file}\")"
   ],
   "id": "b59a3251caaffbc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on full dataset...\n",
      "Testing pipeline on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 15:47:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LogisticRegression_Pipeline' already exists. Creating a new version of this model...\n",
      "2025/04/23 15:47:44 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression_Pipeline, version 7\n",
      "Created version '7' of model 'LogisticRegression_Pipeline'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model registered as: LogisticRegression_Pipeline\n",
      "Submission file saved as: logistic_regression_submission_20250423_1547.csv\n",
      "🏃 View run LogisticRegression_Final_Training at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/801ad630fecf4378848290d423e050f0\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a08aa3c6474cfcc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature analyisis",
   "id": "932200d4898cef22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T11:47:53.796435Z",
     "start_time": "2025-04-23T11:47:52.419736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Feature_Analysis\") as run:\n",
    "    try:\n",
    "        lr_model = final_pipeline.named_steps['classifier']\n",
    "        \n",
    "        if hasattr(lr_model, 'coef_'):\n",
    "            preprocessor = final_pipeline.named_steps['preprocessor']\n",
    "            X_sample = preprocessor.transform(X_train.head(1))\n",
    "            \n",
    "            try:\n",
    "                feature_names = preprocessor.get_feature_names_out()\n",
    "            except:\n",
    "                feature_names = [f\"feature_{i}\" for i in range(X_sample.shape[1])]\n",
    "            \n",
    "            coefficients = np.abs(lr_model.coef_[0])\n",
    "            \n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': coefficients\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "            plt.title('Top 20 Features by Importance')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            importance_plot = \"logistic_regression_feature_importance.png\"\n",
    "            plt.savefig(importance_plot)\n",
    "            mlflow.log_artifact(importance_plot)\n",
    "            \n",
    "            mlflow.log_param(\"top_features\", importance_df['Feature'].head(20).tolist())\n",
    "            mlflow.log_param(\"top_feature_importances\", importance_df['Importance'].head(20).tolist())\n",
    "            \n",
    "            print(\"Feature importance analysis complete.\")\n",
    "        else:\n",
    "            print(\"Model does not have coefficient attributes.\")\n",
    "            mlflow.log_param(\"feature_importance_analysis\", \"model_has_no_coefficients\")\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance analysis failed: {e}\")\n",
    "        mlflow.log_param(\"feature_importance_analysis\", \"failed\")"
   ],
   "id": "8f6b33ef6a7b04cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance analysis failed: All arrays must be of the same length\n",
      "🏃 View run LogisticRegression_Feature_Analysis at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1/runs/9ddc65cc62814ac0a531a898a91875df\n",
      "🧪 View experiment at: https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5f62af0afddd024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
