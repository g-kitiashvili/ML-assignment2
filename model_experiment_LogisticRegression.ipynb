{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:14.792371Z",
     "start_time": "2025-04-22T15:56:12.854499Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -q dagshub mlflow",
   "id": "29e14893778177b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Import/setup",
   "id": "b0436d2febd11ca7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:18.699158Z",
     "start_time": "2025-04-22T15:56:14.812748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'g-kitiashvili'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '1c2227158cc19daf66bb3b241116a8e8c5f1cd20' \n",
    "\n",
    "model_name = \"LogisticRegression\" \n",
    "mlflow.set_experiment(f\"{model_name}_Training\")\n"
   ],
   "id": "95a40e6bde682f24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/4638122e69014004ab70824f7a114f4b', creation_time=1745256286391, experiment_id='1', last_update_time=1745256286391, lifecycle_stage='active', name='LogisticRegression_Training', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "\n",
    "Custom Transformers for Pipeline"
   ],
   "id": "e6157af3b571c3b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:18.838977Z",
     "start_time": "2025-04-22T15:56:18.817931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Feature selection using SelectKBest\"\"\"\n",
    "    \n",
    "    def __init__(self, k=100):\n",
    "        self.k = k\n",
    "        self.selector = None\n",
    "        self.selected_feature_indices = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.selector = SelectKBest(f_classif, k=min(self.k, X.shape[1]))\n",
    "        self.selector.fit(X, y)\n",
    "        self.selected_feature_indices = self.selector.get_support()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handle missing values and remove high-missing columns\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=50):\n",
    "        self.threshold = threshold\n",
    "        self.high_missing_cols = None\n",
    "        self.constant_features = None\n",
    "        self.medians = {}\n",
    "        self.modes = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Identify columns with too many missing values\n",
    "        missing_percent = (X.isnull().mean() * 100)\n",
    "        self.high_missing_cols = missing_percent[missing_percent > self.threshold].index.tolist()\n",
    "        \n",
    "        # Identify constant features\n",
    "        self.constant_features = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "        \n",
    "        # Calculate median for numerical features\n",
    "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            if col not in self.high_missing_cols and col not in self.constant_features:\n",
    "                self.medians[col] = X[col].median()\n",
    "        \n",
    "        # Calculate mode for categorical features\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if col not in self.high_missing_cols and col not in self.constant_features:\n",
    "                self.modes[col] = X[col].mode()[0] if not X[col].mode().empty else 'missing'\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Remove high missing columns\n",
    "        X_new = X_new.drop(columns=self.high_missing_cols, errors='ignore')\n",
    "        \n",
    "        # Remove constant features\n",
    "        X_new = X_new.drop(columns=self.constant_features, errors='ignore')\n",
    "        \n",
    "        # Fill missing values in numerical features\n",
    "        for col, median in self.medians.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[col] = X_new[col].fillna(median)\n",
    "        \n",
    "        # Fill missing values in categorical features\n",
    "        for col, mode in self.modes.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[col] = X_new[col].fillna(mode)\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handle outliers for numerical features\"\"\"\n",
    "    \n",
    "    def __init__(self, q_low=0.01, q_high=0.99):\n",
    "        self.q_low = q_low\n",
    "        self.q_high = q_high\n",
    "        self.bounds = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            if 'TransactionDT' not in col and col in X.columns:\n",
    "                q1 = X[col].quantile(self.q_low)\n",
    "                q3 = X[col].quantile(self.q_high)\n",
    "                self.bounds[col] = (q1, q3)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        for col, (q1, q3) in self.bounds.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[col] = np.clip(X_new[col], q1, q3)\n",
    "        return X_new\n",
    "\n",
    "class DatetimeFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transform TransactionDT into useful features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'TransactionDT' in X_new.columns:\n",
    "            # Convert to days\n",
    "            X_new['TransactionDay'] = X_new['TransactionDT'] / (24 * 60 * 60)\n",
    "            \n",
    "            # Create cyclical features for day of week\n",
    "            X_new['DayOfWeek_sin'] = np.sin(2 * np.pi * (X_new['TransactionDay'] % 7) / 7)\n",
    "            X_new['DayOfWeek_cos'] = np.cos(2 * np.pi * (X_new['TransactionDay'] % 7) / 7)\n",
    "            \n",
    "            # Create hour of day\n",
    "            X_new['Hour'] = (X_new['TransactionDT'] % (24 * 60 * 60)) / 3600\n",
    "            \n",
    "            # Cyclical features for hour\n",
    "            X_new['Hour_sin'] = np.sin(2 * np.pi * X_new['Hour'] / 24)\n",
    "            X_new['Hour_cos'] = np.cos(2 * np.pi * X_new['Hour'] / 24)\n",
    "            \n",
    "            # Drop original\n",
    "            X_new = X_new.drop(['TransactionDT', 'Hour'], axis=1)\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply log transformation to skewed features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'TransactionAmt' in X_new.columns:\n",
    "            X_new['TransactionAmt_Log'] = np.log1p(X_new['TransactionAmt'])\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class CategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features\"\"\"\n",
    "    \n",
    "    def __init__(self, max_categories=20):\n",
    "        self.max_categories = max_categories\n",
    "        self.encodings = {}\n",
    "        self.frequency_maps = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if X[col].nunique() < self.max_categories:\n",
    "                # Get dummies for low cardinality\n",
    "                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n",
    "                self.encodings[col] = dummies.columns.tolist()\n",
    "            else:\n",
    "                # Frequency encoding for high cardinality\n",
    "                self.frequency_maps[col] = X[col].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Apply encodings\n",
    "        for col, dummy_cols in self.encodings.items():\n",
    "            if col in X_new.columns:\n",
    "                # Create one-hot encoding\n",
    "                dummies = pd.get_dummies(X_new[col], prefix=col, drop_first=True)\n",
    "                \n",
    "                # Ensure all expected columns exist\n",
    "                for dummy_col in dummy_cols:\n",
    "                    if dummy_col not in dummies.columns:\n",
    "                        dummies[dummy_col] = 0\n",
    "                \n",
    "                # Only keep columns from training\n",
    "                dummies = dummies[dummy_cols]\n",
    "                \n",
    "                # Add to dataframe\n",
    "                X_new = pd.concat([X_new, dummies], axis=1)\n",
    "                \n",
    "                # Drop original\n",
    "                X_new = X_new.drop(columns=[col])\n",
    "        \n",
    "        # Apply frequency encoding\n",
    "        for col, freq_map in self.frequency_maps.items():\n",
    "            if col in X_new.columns:\n",
    "                X_new[f'{col}_freq'] = X_new[col].map(freq_map).fillna(0)\n",
    "                X_new = X_new.drop(columns=[col])\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class InteractionFeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create interaction features between important variables\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Create card interactions\n",
    "        card_cols = [col for col in X_new.columns if 'card' in col.lower() \n",
    "                     and X_new[col].dtype != 'object' and not pd.api.types.is_bool_dtype(X_new[col])]\n",
    "        \n",
    "        if len(card_cols) >= 2:\n",
    "            for i in range(min(3, len(card_cols)-1)):  # Limit to 3 to prevent explosion\n",
    "                for j in range(i+1, min(i+3, len(card_cols))):\n",
    "                    col_name = f\"{card_cols[i]}_x_{card_cols[j]}\"\n",
    "                    X_new[col_name] = X_new[card_cols[i]] * X_new[card_cols[j]]\n",
    "        \n",
    "        # Create addr interactions\n",
    "        addr_cols = [col for col in X_new.columns if 'addr' in col.lower() \n",
    "                     and X_new[col].dtype != 'object' and not pd.api.types.is_bool_dtype(X_new[col])]\n",
    "        \n",
    "        if len(addr_cols) >= 2:\n",
    "            for i in range(min(3, len(addr_cols)-1)):\n",
    "                for j in range(i+1, min(i+3, len(addr_cols))):\n",
    "                    col_name = f\"{addr_cols[i]}_x_{addr_cols[j]}\"\n",
    "                    X_new[col_name] = X_new[addr_cols[i]] * X_new[addr_cols[j]]\n",
    "        \n",
    "        # Create email domain interactions\n",
    "        email_cols = [col for col in X_new.columns if 'email' in col.lower() \n",
    "                      and X_new[col].dtype != 'object' and not pd.api.types.is_bool_dtype(X_new[col])]\n",
    "        \n",
    "        if len(email_cols) >= 2:\n",
    "            for i in range(len(email_cols)-1):\n",
    "                for j in range(i+1, len(email_cols)):\n",
    "                    col_name = f\"{email_cols[i]}_x_{email_cols[j]}\"\n",
    "                    X_new[col_name] = X_new[email_cols[i]] * X_new[email_cols[j]]\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Remove highly correlated features\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.9):\n",
    "        self.threshold = threshold\n",
    "        self.drop_cols = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = X.corr().abs()\n",
    "        \n",
    "        # Get upper triangle\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features with correlation greater than threshold\n",
    "        self.drop_cols = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.drop_cols, errors='ignore')\n"
   ],
   "id": "8072e65d8ce01371",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": " Data Loading and Preparation\n",
   "id": "a44334086aa3fd86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:19.575801Z",
     "start_time": "2025-04-22T15:56:18.883471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "# Load transaction data\n",
    "train_transaction = pd.read_csv('./data/train_transaction.csv')\n",
    "test_transaction = pd.read_csv('./data/test_transaction.csv')\n",
    "\n",
    "# Load identity data  \n",
    "train_identity = pd.read_csv('./data/train_identity.csv')\n",
    "test_identity = pd.read_csv('./data/test_identity.csv')\n",
    "\n",
    "print(f\"Train transaction shape: {train_transaction.shape}\")\n",
    "print(f\"Test transaction shape: {test_transaction.shape}\")\n",
    "print(f\"Train identity shape: {train_identity.shape}\")\n",
    "print(f\"Test identity shape: {test_identity.shape}\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Initial_Preparation\") as run:\n",
    "    print(\"Merging data...\")\n",
    "    \n",
    "    train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "    test = test_transaction.merge(test_identity, on='TransactionID', how='left')\n",
    "    \n",
    "    mlflow.log_param(\"train_original_shape\", train.shape)\n",
    "    mlflow.log_param(\"test_original_shape\", test.shape)\n",
    "    \n",
    "    del train_transaction, train_identity\n",
    "    gc.collect()\n",
    "    \n",
    "    target = 'isFraud'\n",
    "    y_train = train[target].copy()\n",
    "    train_transaction_id = train['TransactionID'].copy()\n",
    "    test_transaction_id = test['TransactionID'].copy()\n",
    "    \n",
    "    fraud_ratio = y_train.mean()\n",
    "    mlflow.log_param(\"fraud_ratio\", fraud_ratio)\n",
    "    print(f\"Fraud ratio: {fraud_ratio:.4f}\")\n",
    "    \n",
    "    X_train = train.drop(['isFraud'], axis=1)\n",
    "    \n",
    "    del train\n",
    "    gc.collect()"
   ],
   "id": "cea77ac29ef80951",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/ieee-fraud-detection/train_transaction.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mLoading data...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Load transaction data\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m train_transaction = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m/kaggle/input/ieee-fraud-detection/train_transaction.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      5\u001B[39m test_transaction = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m/kaggle/input/ieee-fraud-detection/test_transaction.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Load identity data  \u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28mself\u001B[39m._make_engine(f, \u001B[38;5;28mself\u001B[39m.engine)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = get_handle(\n\u001B[32m   1881\u001B[39m     f,\n\u001B[32m   1882\u001B[39m     mode,\n\u001B[32m   1883\u001B[39m     encoding=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1884\u001B[39m     compression=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mcompression\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1885\u001B[39m     memory_map=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mmemory_map\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[32m   1886\u001B[39m     is_text=is_text,\n\u001B[32m   1887\u001B[39m     errors=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mencoding_errors\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstrict\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1888\u001B[39m     storage_options=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mstorage_options\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1889\u001B[39m )\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(\n\u001B[32m    874\u001B[39m             handle,\n\u001B[32m    875\u001B[39m             ioargs.mode,\n\u001B[32m    876\u001B[39m             encoding=ioargs.encoding,\n\u001B[32m    877\u001B[39m             errors=errors,\n\u001B[32m    878\u001B[39m             newline=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    879\u001B[39m         )\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/kaggle/input/ieee-fraud-detection/train_transaction.csv'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    " \n",
    " Data Exploration for Pipeline Development\n"
   ],
   "id": "6b35f443220cd30d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Exploration\") as run:\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    numerical_features.remove('TransactionID')  # Remove ID column\n",
    "    \n",
    "    # Log feature types\n",
    "    mlflow.log_param(\"categorical_features_count\", len(categorical_features))\n",
    "    mlflow.log_param(\"numerical_features_count\", len(numerical_features))\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = X_train.isnull().mean() * 100\n",
    "    high_missing_cols = missing_values[missing_values > 50].index.tolist()\n",
    "    \n",
    "    mlflow.log_param(\"high_missing_cols_count\", len(high_missing_cols))\n",
    "    \n",
    "    mlflow.log_param(\"transaction_amount_mean\", X_train['TransactionAmt'].mean())\n",
    "    mlflow.log_param(\"transaction_amount_std\", X_train['TransactionAmt'].std())\n",
    "    \n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    print(f\"High missing columns: {len(high_missing_cols)}\")"
   ],
   "id": "d866c8b200fc4372"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6a2b0b513ec56900"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating pipeline",
   "id": "654e852c8242440"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Pipeline_Testing\") as run:\n",
    "    # Test pipeline steps individually to ensure they work\n",
    "    \n",
    "    print(\"Testing MissingValueHandler...\")\n",
    "    missing_handler = MissingValueHandler(threshold=50)\n",
    "    missing_handler.fit(X_train)\n",
    "    X_sample = missing_handler.transform(X_train.iloc[:1000])\n",
    "    mlflow.log_param(\"missing_handler_removed_cols\", len(missing_handler.high_missing_cols) + len(missing_handler.constant_features))\n",
    "    \n",
    "    print(\"Testing DatetimeFeatureTransformer...\")\n",
    "    dt_transformer = DatetimeFeatureTransformer()\n",
    "    X_sample = dt_transformer.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing LogTransformer...\")\n",
    "    log_transformer = LogTransformer()\n",
    "    X_sample = log_transformer.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing OutlierHandler...\")\n",
    "    outlier_handler = OutlierHandler()\n",
    "    outlier_handler.fit(X_train)\n",
    "    X_sample = outlier_handler.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing CategoryEncoder...\")\n",
    "    cat_encoder = CategoryEncoder()\n",
    "    cat_encoder.fit(X_train)\n",
    "    X_sample = cat_encoder.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing InteractionFeatureCreator...\")\n",
    "    interaction_creator = InteractionFeatureCreator()\n",
    "    X_sample = interaction_creator.transform(X_sample)\n",
    "    \n",
    "    print(\"Testing CorrelationFilter...\")\n",
    "    corr_filter = CorrelationFilter()\n",
    "    try:\n",
    "        corr_filter.fit(X_sample)\n",
    "        X_sample = corr_filter.transform(X_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"CorrelationFilter test error: {e}\")\n",
    "    \n",
    "    print(\"Testing FeatureSelector...\")\n",
    "    feature_selector = FeatureSelector(k=100)\n",
    "    try:\n",
    "        y_sample = y_train.iloc[:1000]\n",
    "        feature_selector.fit(X_sample, y_sample)\n",
    "        X_sample = feature_selector.transform(X_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"FeatureSelector test error: {e}\")\n",
    "    \n",
    "    mlflow.log_param(\"pipeline_testing_successful\", True)\n",
    "    mlflow.log_param(\"sample_features_after_transforms\", X_sample.shape[1])\n",
    "\n",
    "    \n",
    "def create_logistic_regression_pipeline(C=1.0, penalty='l2', class_weight='balanced', \n",
    "                                      solver='saga', l1_ratio=0.5, max_iter=1000):\n",
    "    preprocessor = Pipeline([\n",
    "        ('missing_handler', MissingValueHandler(threshold=50)),\n",
    "        ('datetime_transformer', DatetimeFeatureTransformer()),\n",
    "        ('log_transformer', LogTransformer()),\n",
    "        ('outlier_handler', OutlierHandler(q_low=0.01, q_high=0.99)),\n",
    "        ('category_encoder', CategoryEncoder(max_categories=20)),\n",
    "        ('interaction_creator', InteractionFeatureCreator()),\n",
    "        ('corr_filter', CorrelationFilter(threshold=0.9)),\n",
    "    ])\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        class_weight=class_weight,\n",
    "        solver=solver,\n",
    "        l1_ratio=l1_ratio if penalty == 'elasticnet' else None,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create the final pipeline including feature selection\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selector', FeatureSelector(k=100)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    return full_pipeline"
   ],
   "id": "7261ddebd42af02f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "\n",
    "\n",
    "Data Preprocessing and training"
   ],
   "id": "10b9f50bcd503b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(X_train, y_train, X_val=None, y_val=None, C=1.0, penalty='l2', max_iter=1000):\n",
    "    \n",
    "    preprocessor = Pipeline([\n",
    "        ('missing_handler', MissingValueHandler(threshold=50)),\n",
    "        ('datetime_transformer', DatetimeFeatureTransformer()),\n",
    "        ('log_transformer', LogTransformer()),\n",
    "        ('outlier_handler', OutlierHandler(q_low=0.01, q_high=0.99)),\n",
    "        ('category_encoder', CategoryEncoder(max_categories=20)),\n",
    "        ('interaction_creator', InteractionFeatureCreator()),\n",
    "        ('corr_filter', CorrelationFilter(threshold=0.9)),\n",
    "    ])\n",
    "    \n",
    "    # Preprocess with sklearn\n",
    "    print(\"Preprocessing data with sklearn pipeline...\")\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_val_preprocessed = preprocessor.transform(X_val) if X_val is not None else None\n",
    "    \n",
    "    # Apply scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_preprocessed)\n",
    "    X_val_scaled = scaler.transform(X_val_preprocessed) if X_val_preprocessed is not None else None\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = FeatureSelector(k=100)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_val_selected = selector.transform(X_val_scaled) if X_val_scaled is not None else None\n",
    "    \n",
    "\n",
    "        \n",
    "        # Fallback to CPU LogisticRegression\n",
    "    model = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty if penalty != 'elasticnet' else 'l1',  # elasticnet not supported without saga\n",
    "            solver='saga' if penalty == 'elasticnet' else 'liblinear',\n",
    "            l1_ratio=0.5 if penalty == 'elasticnet' else None,\n",
    "            max_iter=max_iter,\n",
    "            random_state=42\n",
    "        )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "        \n",
    "    if X_val_selected is not None:\n",
    "        y_val_pred = model.predict_proba(X_val_selected)[:, 1]\n",
    "    \n",
    "    if X_val_selected is not None:\n",
    "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_val_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        print(f\"Validation PR-AUC: {pr_auc:.4f}\")\n",
    "        \n",
    "        return model, preprocessor, scaler, selector, val_auc, pr_auc\n",
    "    \n",
    "    return model, preprocessor, scaler, selector, None, None"
   ],
   "id": "cc77afe6320f99fe"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "CrossValidation and tuning",
   "id": "5535f27294736842"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Cross_Validation\") as run:\n",
    "    # Split data for validation\n",
    "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Log data split info\n",
    "    mlflow.log_param(\"validation_split\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"train_size\", X_train_cv.shape[0])\n",
    "    mlflow.log_param(\"validation_size\", X_val.shape[0])\n",
    "    \n",
    "    # Train with GPU\n",
    "    model, preprocessor, scaler, selector, val_auc, pr_auc = train(\n",
    "        X_train_cv, y_train_cv, X_val, y_val, C=1.0, penalty='l2', max_iter=1000\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"validation_auc\", val_auc)\n",
    "    mlflow.log_metric(\"validation_pr_auc\", pr_auc)\n",
    "    \n",
    "    # We can't log the cuML model directly with mlflow.sklearn\n",
    "    # Instead, log important parameters\n",
    "    mlflow.log_param(\"model_type\", \"cuML_LogisticRegression\")\n",
    "    mlflow.log_param(\"C\", 1.0)\n",
    "    mlflow.log_param(\"penalty\", \"l2\")\n",
    "    mlflow.log_param(\"max_iter\", 1000)\n",
    "    \n",
    "    print(f\"GPU-accelerated training complete.\")\n",
    "    print(f\"AUC: {val_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "# Hyperparameter Tuning with GPU\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Hyperparameter_Tuning\") as run:\n",
    "    # Define parameters to tune\n",
    "    C_values = [0.01, 0.1, 1.0, 10.0]\n",
    "    \n",
    "    # Log tuning parameters\n",
    "    mlflow.log_param(\"tuning_C_values\", C_values)\n",
    "    \n",
    "    # Use a smaller subset for faster tuning\n",
    "    X_tune, X_val_tune, y_tune, y_val_tune = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Manual grid search\n",
    "    best_auc = 0\n",
    "    best_C = None\n",
    "    best_components = None\n",
    "    \n",
    "    for C in C_values:\n",
    "        print(f\"Testing C={C}...\")\n",
    "        \n",
    "        # Train with GPU\n",
    "        model, preprocessor, scaler, selector, val_auc, pr_auc = train(\n",
    "            X_tune, y_tune, X_val_tune, y_val_tune, C=C, penalty='l2', max_iter=1000\n",
    "        )\n",
    "        \n",
    "        # Log metrics for this iteration\n",
    "        mlflow.log_metric(f\"C_{C}_auc\", val_auc)\n",
    "        mlflow.log_metric(f\"C_{C}_pr_auc\", pr_auc)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_C = C\n",
    "            best_components = (model, preprocessor, scaler, selector)\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_param(\"best_C\", best_C)\n",
    "    mlflow.log_metric(\"best_cv_auc\", best_auc)\n",
    "    best_params = {\n",
    "    'classifier__C': best_C,\n",
    "    'classifier__penalty': 'l2', \n",
    "    'classifier__class_weight': 'balanced'  \n",
    "    \n",
    "}\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best C value: {best_C}\")\n",
    "    print(f\"Best CV AUC: {best_auc:.4f}\")\n"
   ],
   "id": "83f82ba2f2a02377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a8ffb46fb02bb726"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Final Training with Full Dataset\n",
   "id": "dc5cfc7bab0ffdf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Final_Training\") as run:\n",
    "    # Check if best_params exists, if not use default values\n",
    "    try:\n",
    "        final_params = {param.replace('classifier__', ''): value \n",
    "                       for param, value in best_params.items()}\n",
    "    except NameError:\n",
    "        print(\"best_params not found. Using default parameters.\")\n",
    "        final_params = {\n",
    "            'C': 1.0,\n",
    "            'penalty': 'l2',\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "    \n",
    "    mlflow.log_params(final_params)\n",
    "    \n",
    "  \n",
    "    final_pipeline = create_logistic_regression_pipeline(\n",
    "        C=final_params.get('C', 1.0),\n",
    "        penalty=final_params.get('penalty', 'l2'),\n",
    "        class_weight=final_params.get('class_weight', 'balanced'),\n",
    "        solver='saga',\n",
    "        l1_ratio=0.5 if final_params.get('penalty') == 'elasticnet' else None,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    print(\"Training final model on full dataset...\")\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    try:\n",
    "        # Testing pipeline on test data\n",
    "        print(\"Testing pipeline on test data...\")\n",
    "        \n",
    "        original_test_columns = test.columns.tolist()\n",
    "        \n",
    "        columns_to_keep = [col for col in original_test_columns if col in X_train.columns]\n",
    "        test_matched = test[columns_to_keep]\n",
    "        \n",
    "        for col in X_train.columns:\n",
    "            if col not in test_matched.columns:\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    test_matched[col] = X_train[col].median()\n",
    "                else:\n",
    "                    test_matched[col] = X_train[col].mode()[0]\n",
    "        \n",
    "        test_probs = final_pipeline.predict_proba(test_matched)[:, 1]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        print(\"Falling back to basic prediction approach\")\n",
    "        simple_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(C=1.0, max_iter=1000))\n",
    "        ])\n",
    "        \n",
    "        numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "        X_train_simple = X_train[numeric_cols].copy()\n",
    "        test_simple = test[numeric_cols.intersection(test.columns)].copy()\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col not in test_simple.columns:\n",
    "                test_simple[col] = 0\n",
    "        \n",
    "        test_simple = test_simple[X_train_simple.columns]\n",
    "        \n",
    "        # Fill NaN values\n",
    "        X_train_simple.fillna(0, inplace=True)\n",
    "        test_simple.fillna(0, inplace=True)\n",
    "        \n",
    "        simple_pipeline.fit(X_train_simple, y_train)\n",
    "        test_probs = simple_pipeline.predict_proba(test_simple)[:, 1]\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'TransactionID': test_transaction_id,\n",
    "        'isFraud': test_probs\n",
    "    })\n",
    "    \n",
    "    submission_file = f\"logistic_regression_submission_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    \n",
    "    mlflow.log_artifact(submission_file)\n",
    "    \n",
    "    mlflow.sklearn.log_model(final_pipeline, \"final_pipeline\")\n",
    "    \n",
    "    model_registry_name = f\"{model_name}_Pipeline\"\n",
    "    model_description = f\"Full {model_name} pipeline including all preprocessing steps\"\n",
    "    \n",
    "    try:\n",
    "        mlflow.register_model(\n",
    "            f\"runs:/{run.info.run_id}/final_pipeline\",\n",
    "            model_registry_name,\n",
    "            tags={\"description\": model_description}\n",
    "        )\n",
    "        print(f\"Final model registered as: {model_registry_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error registering model: {e}\")\n",
    "    \n",
    "    print(f\"Submission file saved as: {submission_file}\")"
   ],
   "id": "b59a3251caaffbc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a08aa3c6474cfcc9"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Feature analyisis",
   "id": "158d3f051d9dc8e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature Importance Analysis\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Feature_Analysis\") as run:\n",
    "    # Extract feature names and coefficients after preprocessing\n",
    "    try:\n",
    "        # Try to extract model\n",
    "        lr_model = final_pipeline.named_steps['classifier']\n",
    "        \n",
    "        # Check if the model has coefficients (LogisticRegression would)\n",
    "        if hasattr(lr_model, 'coef_'):\n",
    "            # Get coefficients\n",
    "            coefficients = lr_model.coef_[0]\n",
    "            \n",
    "            # Create a dataframe of feature importance\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature_Index': range(len(coefficients)),\n",
    "                'Importance': np.abs(coefficients)\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature_Index', data=importance_df.head(20))\n",
    "            plt.title('Top 20 Features by Importance')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save and log the figure\n",
    "            importance_plot = \"feature_importance.png\"\n",
    "            plt.savefig(importance_plot)\n",
    "            mlflow.log_artifact(importance_plot)\n",
    "            \n",
    "            # Try to get the actual feature names if possible\n",
    "            try:\n",
    "                # Attempt to get the feature names from the pipeline\n",
    "                # This is complex and may not always work\n",
    "                feature_names = []\n",
    "                \n",
    "                # Log top feature indices for reference\n",
    "                mlflow.log_param(\"top_feature_indices\", importance_df['Feature_Index'].head(20).tolist())\n",
    "                mlflow.log_param(\"top_feature_importances\", importance_df['Importance'].head(20).tolist())\n",
    "                \n",
    "                print(\"Feature importance analysis complete.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not determine feature names: {e}\")\n",
    "                print(\"Using feature indices instead.\")\n",
    "        else:\n",
    "            print(\"Model does not have coefficient attributes.\")\n",
    "            mlflow.log_param(\"feature_importance_analysis\", \"model_has_no_coefficients\")\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance analysis failed: {e}\")\n",
    "        mlflow.log_param(\"feature_importance_analysis\", \"failed\")"
   ],
   "id": "8f6b33ef6a7b04cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
