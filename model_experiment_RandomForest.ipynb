{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:14.792371Z",
     "start_time": "2025-04-22T15:56:12.854499Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": "%pip install -q dagshub mlflow",
   "id": "13be50599a6d423f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Import/setup",
   "id": "2161a4e731a78af0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:18.699158Z",
     "start_time": "2025-04-22T15:56:14.812748Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/4638122e69014004ab70824f7a114f4b', creation_time=1745256286391, experiment_id='1', last_update_time=1745256286391, lifecycle_stage='active', name='LogisticRegression_Training', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import gc\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/g-kitiashvili/ML-assignment2.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'g-kitiashvili'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '1c2227158cc19daf66bb3b241116a8e8c5f1cd20'\n",
    "\n",
    "model_name = \"RandomForest\"\n",
    "mlflow.set_experiment(f\"{model_name}_Training\")\n"
   ],
   "id": "e160a7628edc590f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "a41deb17de1b8ab1"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": " Data Loading and Preparation\n",
   "id": "1c50c4f0f0ca7711"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:56:19.575801Z",
     "start_time": "2025-04-22T15:56:18.883471Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/ieee-fraud-detection/train_transaction.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mLoading data...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Load transaction data\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m train_transaction = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m/kaggle/input/ieee-fraud-detection/train_transaction.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      5\u001B[39m test_transaction = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m/kaggle/input/ieee-fraud-detection/test_transaction.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Load identity data  \u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28mself\u001B[39m._make_engine(f, \u001B[38;5;28mself\u001B[39m.engine)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = get_handle(\n\u001B[32m   1881\u001B[39m     f,\n\u001B[32m   1882\u001B[39m     mode,\n\u001B[32m   1883\u001B[39m     encoding=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1884\u001B[39m     compression=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mcompression\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1885\u001B[39m     memory_map=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mmemory_map\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[32m   1886\u001B[39m     is_text=is_text,\n\u001B[32m   1887\u001B[39m     errors=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mencoding_errors\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstrict\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1888\u001B[39m     storage_options=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mstorage_options\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1889\u001B[39m )\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(\n\u001B[32m    874\u001B[39m             handle,\n\u001B[32m    875\u001B[39m             ioargs.mode,\n\u001B[32m    876\u001B[39m             encoding=ioargs.encoding,\n\u001B[32m    877\u001B[39m             errors=errors,\n\u001B[32m    878\u001B[39m             newline=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    879\u001B[39m         )\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/kaggle/input/ieee-fraud-detection/train_transaction.csv'"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "train_transaction = pd.read_csv('./data/train_transaction.csv')\n",
    "test_transaction = pd.read_csv('./data/test_transaction.csv')\n",
    "\n",
    "train_identity = pd.read_csv('./data/train_identity.csv')\n",
    "test_identity = pd.read_csv('./data/test_identity.csv')\n",
    "\n",
    "print(f\"Train transaction shape: {train_transaction.shape}\")\n",
    "print(f\"Test transaction shape: {test_transaction.shape}\")\n",
    "print(f\"Train identity shape: {train_identity.shape}\")\n",
    "print(f\"Test identity shape: {test_identity.shape}\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Initial_Preparation\") as run:\n",
    "    # Merge data sets\n",
    "    print(\"Merging data...\")\n",
    "    \n",
    "    train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "    test = test_transaction.merge(test_identity, on='TransactionID', how='left')\n",
    "    \n",
    "    mlflow.log_param(\"train_original_shape\", train.shape)\n",
    "    mlflow.log_param(\"test_original_shape\", test.shape)\n",
    "    \n",
    "    del train_transaction, train_identity\n",
    "    gc.collect()\n",
    "    \n",
    "    target = 'isFraud'\n",
    "    y_train = train[target].copy()\n",
    "    train_transaction_id = train['TransactionID'].copy()\n",
    "    test_transaction_id = test['TransactionID'].copy()\n",
    "    \n",
    "    fraud_ratio = y_train.mean()\n",
    "    mlflow.log_param(\"fraud_ratio\", fraud_ratio)\n",
    "    print(f\"Fraud ratio: {fraud_ratio:.4f}\")\n",
    "    \n",
    "    X_train = train.drop(['isFraud'], axis=1)\n",
    "    \n",
    "    del train\n",
    "    gc.collect()"
   ],
   "id": "1e9b24335a5eb201"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    " \n",
    " Data Exploration for Pipeline Development\n"
   ],
   "id": "8b432cfe3ec55fc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Exploration\") as run:\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    numerical_features.remove('TransactionID')  # Remove ID column\n",
    "    \n",
    "    mlflow.log_param(\"categorical_features_count\", len(categorical_features))\n",
    "    mlflow.log_param(\"numerical_features_count\", len(numerical_features))\n",
    "    \n",
    "    missing_values = X_train.isnull().mean() * 100\n",
    "    high_missing_cols = missing_values[missing_values > 50].index.tolist()\n",
    "    \n",
    "    mlflow.log_param(\"high_missing_cols_count\", len(high_missing_cols))\n",
    "    \n",
    "    mlflow.log_param(\"transaction_amount_mean\", X_train['TransactionAmt'].mean())\n",
    "    mlflow.log_param(\"transaction_amount_std\", X_train['TransactionAmt'].std())\n",
    "    \n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    print(f\"High missing columns: {len(high_missing_cols)}\")\n"
   ],
   "id": "e9bf9c578094c801"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "837783ececf206f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating pipeline",
   "id": "1b736640848c0001"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class HighMissingFilter:\n",
    "    def __init__(self, threshold=80):\n",
    "        self.threshold = threshold\n",
    "        self.high_missing_cols = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        missing_values = X.isnull().mean() * 100\n",
    "        self.high_missing_cols = missing_values[missing_values > self.threshold].index.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.high_missing_cols, errors='ignore')\n",
    "\n",
    "class ConstantFeatureFilter:\n",
    "    def __init__(self):\n",
    "        self.constant_features = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.constant_features = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.constant_features, errors='ignore')\n",
    "\n",
    "class DatetimeFeatureGenerator:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'TransactionDT' in X_new.columns:\n",
    "            # Convert to relative days\n",
    "            X_new['TransactionDay'] = X_new['TransactionDT'] / (24 * 60 * 60)\n",
    "            \n",
    "            # Convert to day of week (0-6)\n",
    "            X_new['DayOfWeek'] = np.floor(X_new['TransactionDay'] % 7)\n",
    "            \n",
    "            # Convert to hour of day (0-23)\n",
    "            X_new['Hour'] = np.floor((X_new['TransactionDT'] % (24 * 60 * 60)) / 3600)\n",
    "            \n",
    "            # Create weekend flag\n",
    "            X_new['IsWeekend'] = np.where(X_new['DayOfWeek'] >= 5, 1, 0)\n",
    "            \n",
    "            # Drop original timestamp\n",
    "            X_new = X_new.drop(['TransactionDT'], axis=1)\n",
    "            \n",
    "        return X_new\n",
    "\n",
    "class CategoryEncoder:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in X.select_dtypes(include=['object']).columns:\n",
    "            le = LabelEncoder()\n",
    "            # Fit including missing values\n",
    "            values = X[col].fillna('missing').astype(str).values\n",
    "            le.fit(values)\n",
    "            self.encoders[col] = le\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        for col, encoder in self.encoders.items():\n",
    "            if col in X_new.columns:\n",
    "                values = X_new[col].fillna('missing').astype(str).values\n",
    "                \n",
    "                try:\n",
    "                    X_new[col] = encoder.transform(values)\n",
    "                except ValueError as e:\n",
    "                    unknown_mask = ~np.isin(values, encoder.classes_)\n",
    "                    \n",
    "                    if unknown_mask.any():\n",
    "                        print(f\"Warning: Column {col} has {np.sum(unknown_mask)} unseen categories. Treating as 'missing'.\")\n",
    "                        \n",
    "                        values_fixed = values.copy()\n",
    "                        values_fixed[unknown_mask] = 'missing'\n",
    "                        \n",
    "                        try:\n",
    "                            X_new[col] = encoder.transform(values_fixed)\n",
    "                        except ValueError:\n",
    "                            X_new[col] = np.zeros(len(values))\n",
    "                            print(f\"Warning: 'missing' not in classes for {col}. Using zeros.\")\n",
    "        return X_new\n",
    "\n",
    "class MissingValueHandler:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        for col in X_new.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            X_new[col] = X_new[col].fillna(-999)\n",
    "        \n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class InteractionFeatureCreator:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'TransactionAmt' in X_new.columns:\n",
    "            X_new['TransactionAmt_Log'] = np.log1p(X_new['TransactionAmt'])\n",
    "        \n",
    "        if 'P_emaildomain' in X_new.columns and 'R_emaildomain' in X_new.columns:\n",
    "            X_new['email_match'] = (X_new['P_emaildomain'] == X_new['R_emaildomain']).astype(int)\n",
    "        \n",
    "        if 'card1' in X_new.columns:\n",
    "            card1_counts = X_new['card1'].value_counts().to_dict()\n",
    "            X_new['card1_count'] = X_new['card1'].map(card1_counts).fillna(1)\n",
    "        \n",
    "        if 'addr1' in X_new.columns:\n",
    "            addr1_counts = X_new['addr1'].value_counts().to_dict()\n",
    "            X_new['addr1_count'] = X_new['addr1'].map(addr1_counts).fillna(1)\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "class FeatureSelector:\n",
    "    def __init__(self, k=100):\n",
    "        self.k = k\n",
    "        self.selected_features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        importances = rf.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        self.selected_features = X.columns[indices[:self.k]].tolist()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.selected_features]\n",
    "\n",
    "def create_rf_pipeline():\n",
    "    preprocessor = Pipeline([\n",
    "        ('high_missing_filter', HighMissingFilter(threshold=80)),\n",
    "        ('constant_filter', ConstantFeatureFilter()),\n",
    "        ('datetime_features', DatetimeFeatureGenerator()),\n",
    "        ('category_encoder', CategoryEncoder()),\n",
    "        ('missing_handler', MissingValueHandler()),\n",
    "        ('interaction_features', InteractionFeatureCreator())\n",
    "    ])\n",
    "    \n",
    "    rf_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selector', FeatureSelector(k=100)),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return rf_pipeline"
   ],
   "id": "4c2da3097aa5c5cd"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "\n",
    "\n",
    "Data Preprocessing and training"
   ],
   "id": "1adf4449d306f964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(X_train, y_train, X_val=None, y_val=None, n_estimators=100, max_depth=10):\n",
    "    pipeline = create_rf_pipeline()\n",
    "    \n",
    "    pipeline.set_params(\n",
    "        classifier__n_estimators=n_estimators,\n",
    "        classifier__max_depth=max_depth\n",
    "    )\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    if X_val is not None and y_val is not None:\n",
    "        y_val_pred = pipeline.predict_proba(X_val)[:, 1]\n",
    "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_val_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        print(f\"Validation PR-AUC: {pr_auc:.4f}\")\n",
    "        \n",
    "        return pipeline, val_auc, pr_auc\n",
    "    \n",
    "    return pipeline, None, None\n"
   ],
   "id": "61cefc0c1b0ad85c"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "CrossValidation and tuning",
   "id": "5aedc2fb7424ab40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Cross_Validation\") as run:\n",
    "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    mlflow.log_param(\"validation_split\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"train_size\", X_train_cv.shape[0])\n",
    "    mlflow.log_param(\"validation_size\", X_val.shape[0])\n",
    "    \n",
    "    pipeline, val_auc, pr_auc = train(\n",
    "        X_train_cv, y_train_cv, X_val, y_val, \n",
    "        n_estimators=100, max_depth=10\n",
    "    )\n",
    "    \n",
    "    mlflow.log_metric(\"validation_auc\", val_auc)\n",
    "    mlflow.log_metric(\"validation_pr_auc\", pr_auc)\n",
    "    \n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    \n",
    "    print(f\"Cross-validation complete.\")\n",
    "    print(f\"AUC: {val_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Hyperparameter_Tuning\") as run:\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20]\n",
    "    }\n",
    "    \n",
    "    mlflow.log_param(\"tuning_n_estimators\", param_grid['n_estimators'])\n",
    "    mlflow.log_param(\"tuning_max_depth\", param_grid['max_depth'])\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            print(f\"Testing n_estimators={n_estimators}, max_depth={max_depth}...\")\n",
    "            \n",
    "            _, val_auc, pr_auc = train(\n",
    "                X_train_cv, y_train_cv, X_val, y_val,\n",
    "                n_estimators=n_estimators, max_depth=max_depth\n",
    "            )\n",
    "            \n",
    "            mlflow.log_metric(f\"auc_n{n_estimators}_d{max_depth}\", val_auc)\n",
    "            mlflow.log_metric(f\"pr_auc_n{n_estimators}_d{max_depth}\", pr_auc)\n",
    "            \n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "    \n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"best_auc\", best_auc)\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best validation AUC: {best_auc:.4f}\")\n"
   ],
   "id": "da47bef0b0d3584f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f815e53f6602bf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Final Training with Full Dataset\n",
   "id": "5a2273ddb6fd5478"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Final_Training\") as run:\n",
    "    try:\n",
    "        final_params = best_params\n",
    "    except NameError:\n",
    "        print(\"best_params not found. Using default parameters.\")\n",
    "        final_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10\n",
    "        }\n",
    "    \n",
    "    mlflow.log_params(final_params)\n",
    "    \n",
    "    print(\"Training final model on full dataset...\")\n",
    "    final_pipeline = create_rf_pipeline()\n",
    "    final_pipeline.set_params(\n",
    "        classifier__n_estimators=final_params.get('n_estimators', 100),\n",
    "        classifier__max_depth=final_params.get('max_depth', 10)\n",
    "    )\n",
    "    \n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    try:\n",
    "        print(\"Testing pipeline on test data...\")\n",
    "        \n",
    "        original_test_columns = test.columns.tolist()\n",
    "        columns_to_keep = [col for col in original_test_columns if col in X_train.columns]\n",
    "        test_matched = test[columns_to_keep]\n",
    "        \n",
    "        for col in X_train.columns:\n",
    "            if col not in test_matched.columns:\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    test_matched[col] = X_train[col].median()\n",
    "                else:\n",
    "                    test_matched[col] = X_train[col].mode()[0]\n",
    "        \n",
    "        test_probs = final_pipeline.predict_proba(test_matched)[:, 1]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        print(\"Falling back to basic prediction approach\")\n",
    "        \n",
    "        simple_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('classifier', RandomForestClassifier(\n",
    "                n_estimators=100, \n",
    "                max_depth=10,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "        X_train_simple = X_train[numeric_cols].copy()\n",
    "        test_simple = test[numeric_cols.intersection(test.columns)].copy()\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col not in test_simple.columns:\n",
    "                test_simple[col] = 0\n",
    "        \n",
    "        test_simple = test_simple[X_train_simple.columns]\n",
    "        \n",
    "        X_train_simple.fillna(0, inplace=True)\n",
    "        test_simple.fillna(0, inplace=True)\n",
    "        \n",
    "        simple_pipeline.fit(X_train_simple, y_train)\n",
    "        test_probs = simple_pipeline.predict_proba(test_simple)[:, 1]\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'TransactionID': test_transaction_id,\n",
    "        'isFraud': test_probs\n",
    "    })\n",
    "    \n",
    "    submission_file = f\"rf_submission_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    \n",
    "    mlflow.log_artifact(submission_file)\n",
    "    \n",
    "    mlflow.sklearn.log_model(final_pipeline, \"final_pipeline\")\n",
    "    \n",
    "    model_registry_name = f\"{model_name}_Pipeline\"\n",
    "    model_description = f\"Full {model_name} pipeline including all preprocessing steps\"\n",
    "    \n",
    "    try:\n",
    "        mlflow.register_model(\n",
    "            f\"runs:/{run.info.run_id}/final_pipeline\",\n",
    "            model_registry_name,\n",
    "            tags={\"description\": model_description}\n",
    "        )\n",
    "        print(f\"Final model registered as: {model_registry_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error registering model: {e}\")\n",
    "    \n",
    "    print(f\"Submission file saved as: {submission_file}\")\n"
   ],
   "id": "54b7b4a5bf236d01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "72ab4a677c1b413"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Feature analyisis",
   "id": "9699c214b7521157"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with mlflow.start_run(run_name=f\"{model_name}_Feature_Analysis\") as run:\n",
    "    try:\n",
    "        rf_model = final_pipeline.named_steps['classifier']\n",
    "        \n",
    "        if hasattr(rf_model, 'feature_importances_'):\n",
    "            importances = rf_model.feature_importances_\n",
    "            \n",
    "            try:\n",
    "                feature_indices = range(len(importances))\n",
    "                \n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature_Index': feature_indices,\n",
    "                    'Importance': importances\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.barplot(x='Importance', y='Feature_Index', data=importance_df.head(20))\n",
    "                plt.title('Top 20 Features by Importance')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                importance_plot = \"rf_feature_importance.png\"\n",
    "                plt.savefig(importance_plot)\n",
    "                mlflow.log_artifact(importance_plot)\n",
    "                \n",
    "                mlflow.log_param(\"top_feature_indices\", importance_df['Feature_Index'].head(20).tolist())\n",
    "                mlflow.log_param(\"top_feature_importances\", importance_df['Importance'].head(20).tolist())\n",
    "                \n",
    "                print(\"Feature importance analysis complete.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting feature names: {e}\")\n",
    "        else:\n",
    "            print(\"Model does not have feature_importances_ attribute.\")\n",
    "            mlflow.log_param(\"feature_importance_analysis\", \"model_has_no_importances\")\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance analysis failed: {e}\")\n",
    "        mlflow.log_param(\"feature_importance_analysis\", \"failed\")"
   ],
   "id": "5f156b812ba41a85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
